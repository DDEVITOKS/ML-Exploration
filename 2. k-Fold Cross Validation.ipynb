{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning - Tugas 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Dengan data iris (dalam bentuk csv, data sudah disediakan), tuliskan kode dalam Python, agar data bisa digunakan dalam setting 5 cross-validation. \n",
        "2. Untuk membaca file csv bisa menggunakan library pandas atau csv reader dari Python.\n",
        "3. Cross validation harus menggunakan native Python, tidak boleh menggunakan library. (Note:  kelas dalam tiap cv harus mendekati balance.)\n",
        "4. Submission berupa Python notebook. Jangan lupa untuk me-run hasil untuk setiap blok kode "
      ],
      "metadata": {
        "id": "c8COO5UZzvA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Import library & Dataset**"
      ],
      "metadata": {
        "id": "qQ3DV-_40AYI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwe3yme6o3zC",
        "outputId": "c5d04cca-7f56-473f-9b53-783da873d850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   species       150 non-null    object \n",
            " 5   class         150 non-null    int64  \n",
            "dtypes: float64(4), int64(1), object(1)\n",
            "memory usage: 7.2+ KB\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from random import seed\n",
        "from random import randrange\n",
        "\n",
        "#Membaca dataset\n",
        "df = pd.read_csv('/content/iris.csv')\n",
        "\n",
        "#Menambahkan Class untuk setiap species\n",
        "df['class'] = df['species']\n",
        "df['class'] = df['class'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2})\n",
        "df[\"class\"].unique()\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Logistic Function**"
      ],
      "metadata": {
        "id": "flYsd1oospX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span id=\"5\"></span>Untuk mendapatkan hasil antara 0 dan 1, sebuah fungsi, yang disebut **sigmoid**, digunakan untuk mengubah fungsi hipotesis kita. Ini didefinisikan sebagai\n",
        "$$ $$\n",
        "$$h_{\\theta}(x) = g(\\theta^{T} x)$$ \n",
        "$$ $$\n",
        "dimana $h_{\\theta}(x)$ adalah fungsi hipotesis, $x$ adalah catatan tunggal dan \n",
        "$$ $$\n",
        "$$g(z)=\\dfrac{1}{1+e^{-z}}$$\n",
        "$$ $$\n",
        "Dengan menggunakan $g(\\theta^{T} x)$, kita memperoleh probabilitas dan jika $h_{\\theta}(x) \\geq 0.5$, kita mendapatkan $y=1$; jika $h_{\\theta}(x) < 0.5$, kita mendapatkan $y=0$. Selanjutnya, ketika $z \\geq 0$, $g(z) \\geq 0.5$ adalah detail lainnya. Jadi, jika $\\theta^{T} x \\geq 0$, maka $y=1$."
      ],
      "metadata": {
        "id": "CDWgnejLwHIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi Sigmoid \n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "H7-G8Tb3xmfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita tidak dapat menggunakan fungsi biaya yang sama dengan yang kami gunakan untuk regresi linier karena fungsi logistik akan menyebabkan output menjadi bergelombang, menyebabkan banyak local optima. Dengan kata lain, itu bukan fungsi cembung. Itu sebabnya kita perlu mendefinisikan fungsi cost yang berbeda untuk regresi logistik. Ini secara sederhana didefinisikan sebagai\n",
        "$$ $$\n",
        "$$J(\\theta) = \\dfrac{1}{m} \\sum^{m}_{i=1}Cost(h_{\\theta}(x^{(i)}), y^{(i )})$$\n",
        "$$ $$\n",
        "di mana\n",
        "$$ $$\n",
        "$$Cost(h_{\\theta}(x^{(i)}), y^{(i)})=-y^{(i)} \\; log(h_{\\theta}(x^{(i)}))-(1-y^{(i)}) \\; log(1-j_{\\theta}(x^{(i)}))$$\n",
        "$$ $$\n",
        "Sebagai pemeriksaan kewarasan, $J(\\theta)$ dapat diplot atau dicetak sebagai fungsi dari jumlah iterasi untuk memastikan bahwa $J(\\theta)$ **berkurang pada setiap iterasi**, yang menunjukkan bahwa itu konvergen dengan benar. Pada titik ini, pilihan $\\alpha$ penting. Jika kita memilih nilai $\\alpha$ yang tinggi atau kecil, kita mungkin mengalami masalah tentang konvergensi.<span id=\"6\"></span>"
      ],
      "metadata": {
        "id": "1f8rJAdJxRK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi Cost\n",
        "def J(h, y):\n",
        "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()"
      ],
      "metadata": {
        "id": "NT8iRW6Jxoow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk menemukan nilai $\\theta$ yang meminimalkan fungsi cost, saya menggunakan penurunan gradien yang ditunjukkan secara matematis sebagai\n",
        "$$ $$\n",
        "$$\\textbf{Repeat}\\{ \\; \\theta_{j}:= \\theta_{j}-\\alpha \\dfrac{\\partial}{\\partial \\theta_{j}}J(\\theta) \\; \\} \\; di mana \\; j \\in \\{0,1,2,...,n \\}$$\n",
        "$$ $$\n",
        "$$atau$$\n",
        "$$ $$\n",
        "$$\\textbf{Repeat}\\{ \\; \\theta_{j}:= \\theta_{j}-\\dfrac{\\alpha}{m} \\sum^{m}_{i=1} (h_{\\theta}(x^{(i)})- y^{(i)}) \\; x_{j}^{(i)} \\; \\} \\; di mana \\; j \\in \\{0,1,2,...,n \\}$$\n",
        "$$ $$\n",
        "Algoritma terlihat identik dengan regresi linier tetapi perlu diketahui bahwa kali ini fungsi $h_{\\theta}(x^{(i)})$ memiliki **definisi berbeda** dan itulah sebabnya, keduanya tidak sama.\n",
        "\n",
        "Saya juga ingin menjelaskan **regularisasi**. Regularisasi dirancang untuk mengatasi masalah overfitting dan undefitting. Untuk memulai dengan **overfitting**, itu berarti varians tinggi dan biasanya disebabkan oleh fungsi rumit yang menciptakan banyak kurva dan sudut yang tidak perlu yang tidak terkait dengan data. Fungsi ini cocok untuk melatih data dengan baik tetapi dapat menyebabkan hasil yang buruk untuk set pengujian. Di sisi lain, **underfitting** berarti varians rendah dan model yang sangat sederhana. Ini juga dapat menyebabkan hasil yang buruk. Dalam situasi ini, kita perlu menyesuaikan fitur secara manual atau menggunakan beberapa algoritma pemilihan model yang membawa beban kerja ekstra. Sebaliknya, ketika kita menerapkan regularisasi, semua fitur disimpan dan model menyesuaikan $\\theta_{j}$. Ini terutama berfungsi ketika kami memiliki banyak fitur yang sedikit berguna.\n",
        "\n",
        "Ketika kita menambahkan regularisasi, fungsi biaya yang baru adalah\n",
        "$$ $$\n",
        "$$J(\\theta) = \\dfrac{1}{m} \\sum^{m}_{i=1}\\left[-y^{(i)} \\; log(h_{\\theta}(x^{(i)}))-(1-y^{(i)}) \\; log(1-h_{\\theta}(x^{(i)}))\\right]+\\dfrac{\\lambda}{2m}\\sum^{n}_{j=1}\\theta^{2} _{j}$$\n",
        "$$ $$\n",
        "\n",
        "Untuk menghitung probabilitas dengan mudah, saya mendefinisikan fungsi prediksi."
      ],
      "metadata": {
        "id": "Fw3WWl6oxpW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung Penurunan Gradien\n",
        "def gradientdescent(X, y, lmd, alpha, num_iter, print_cost):\n",
        "\n",
        "    # Memilih initial values Zero\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    costs = []  \n",
        "    \n",
        "    for i in range(num_iter):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        \n",
        "        # Menambah Formula\n",
        "        reg = lmd / y.size * theta\n",
        "        reg[0] = 0\n",
        "        cost = J(h, y)\n",
        "        \n",
        "        gradient = np.dot(X.T, (h - y)) / y.size + reg\n",
        "        theta = theta - alpha * gradient\n",
        "    \n",
        "        if print_cost and i % 100 == 0: \n",
        "            print('Number of Iterations: ', i, 'Cost : ', cost, 'Theta: ', theta)\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "      \n",
        "    return theta, costs\n",
        "\n",
        "# Fungsi Prediksi\n",
        "def predict(X_test, theta):\n",
        "    z = np.dot(X_test, theta)\n",
        "    return sigmoid(z)"
      ],
      "metadata": {
        "id": "kxjWH302ylvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terakhir, saya mendefinisikan fungsi utama saya untuk regresi logistik. Ketika kami memiliki lebih dari dua class, saya menggunakan metode one vs all (one vs rest). Secara matematis, dapat ditunjukkan sebagai\n",
        "$$ $$\n",
        "$$h_{\\theta}^{(i)}(x)=P(y=i \\; | \\; x;\\theta) \\;\\;\\;\\;\\; (i=1,2,...,n)$$\n",
        "$$ $$\n",
        "di mana $n$ adalah jumlah class. Setelah menghitung persamaan di atas, saya memilih class $i$ yang memaksimalkan $h_{\\theta}^{(i)}(x)$ untuk menentukan kelas.<span id=\"8\"></span>"
      ],
      "metadata": {
        "id": "tAwW5asDyZK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi Logistik\n",
        "def logistic(X_train, y_train, X_test, lmd=0, alpha=0.1, num_iter=30000, print_cost = False):\n",
        "    # Menambah intercept\n",
        "    intercept = np.ones((X_train.shape[0], 1))\n",
        "    X_train = np.concatenate((intercept, X_train), axis=1)\n",
        "    \n",
        "    intercept = np.ones((X_test.shape[0], 1))\n",
        "    X_test = np.concatenate((intercept, X_test), axis=1)\n",
        "\n",
        "    # 1 melawan sisanya\n",
        "    u=set(y_train)\n",
        "    t=[]\n",
        "    allCosts=[]   \n",
        "    for c in u:\n",
        "        # Mengubah label menjadi 0 dan 1\n",
        "        ynew = np.array(y_train == c, dtype = int)\n",
        "        theta_onevsrest, costs_onevsrest = gradientdescent(X_train, ynew, lmd, alpha, num_iter, print_cost)\n",
        "        t.append(theta_onevsrest)\n",
        "        \n",
        "        # Menyimpan costs\n",
        "        allCosts.append(costs_onevsrest)\n",
        "        \n",
        "    # Menghitung probabiltas data test\n",
        "    pred_test = np.zeros((len(u),len(X_test)))\n",
        "    for i in range(len(u)):\n",
        "        pred_test[i,:] = predict(X_test,t[i])\n",
        "    \n",
        "    # Memilih max probability data test\n",
        "    prediction_test = np.argmax(pred_test, axis=0)\n",
        "    \n",
        "    # Menghitung probabilties data train\n",
        "    pred_train = np.zeros((len(u),len(X_train)))\n",
        "    for i in range(len(u)):\n",
        "        pred_train[i,:] = predict(X_train,t[i])\n",
        "    \n",
        "    # Memilih probability data train\n",
        "    prediction_train = np.argmax(pred_train, axis=0)\n",
        "    \n",
        "    d = {\"costs\": allCosts,\n",
        "         \"Y_prediction_test\": prediction_test, \n",
        "         \"Y_prediction_train\" : prediction_train, \n",
        "         \"learning_rate\" : alpha,\n",
        "         \"num_iterations\": num_iter,\n",
        "         \"lambda\": lmd}\n",
        "        \n",
        "    return d"
      ],
      "metadata": {
        "id": "gHUqGLM6qsji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Split Data**"
      ],
      "metadata": {
        "id": "H37yxjn5vXH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_X = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
        "all_y = df['class']\n",
        "\n",
        "df = df[['sepal_length','sepal_width','petal_length','petal_width','class']]\n",
        "\n",
        "# Shuffle dataset\n",
        "shuffle_df = df.sample(frac=1)\n",
        "\n",
        "# Menentukan pembagian menjadi 80% Train, 20% Test\n",
        "train_size = int(0.8 * len(df))\n",
        "\n",
        "# Split dataset \n",
        "train_data = shuffle_df[:train_size]\n",
        "test_data = shuffle_df[train_size:]\n",
        "\n",
        "X_train = train_data[['sepal_length','sepal_width','petal_length','petal_width']]\n",
        "y_train = train_data['class']\n",
        "X_test = test_data[['sepal_length','sepal_width','petal_length','petal_width']]\n",
        "y_test = test_data['class']\n",
        "\n",
        "def transform(i):\n",
        "    if i == 0:\n",
        "        return 'setosa'\n",
        "    if i == 1:\n",
        "        return 'versicolor'\n",
        "    if i == 2:\n",
        "        return 'virginica'"
      ],
      "metadata": {
        "id": "1uLYMwH0rrN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. k-Fold Cross Validation**"
      ],
      "metadata": {
        "id": "almfRG6dvarT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-Fold Cross Validation adalah teknik yang sangat berguna untuk memeriksa seberapa baik kinerja model ketika kita menerapkannya pada data independen. Ini sering digunakan untuk menandai masalah yang disebabkan oleh overfitting dan bias seleksi. Namun, ini membawa beban dan waktu pemrosesan data tambahan.\n",
        "\n",
        "Gambar di bawah menggambarkan validasi silang k-fold. Singkatnya, secara acak membagi data ke k-lipatan, mengambil salah satu lipatan sebagai set pengujian di setiap langkah dan menghitung akurasi.\n",
        "\n",
        "<img src=\"https://i.imgur.com/hq45Jfq.png\" title=\"sumber: imgur.com\" />"
      ],
      "metadata": {
        "id": "d29Po5CmzH47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation_split(dataset, folds):\n",
        "        dataset_split = []\n",
        "        df_copy = dataset\n",
        "        fold_size = int(df_copy.shape[0] / folds)\n",
        "        \n",
        "        # Loop menyimpan fold\n",
        "        for i in range(folds):\n",
        "            fold = []\n",
        "            # Ketika loop menambah elemen ke folds\n",
        "            while len(fold) < fold_size:\n",
        "                # Memilih random element\n",
        "                r = randrange(df_copy.shape[0])\n",
        "                # Menentukan indeks untuk element\n",
        "                index = df_copy.index[r]\n",
        "                # Menyimpan line\n",
        "                fold.append(df_copy.loc[index].values.tolist())\n",
        "                df_copy = df_copy.drop(index)\n",
        "            #  Menyimpan fold\n",
        "            dataset_split.append(np.asarray(fold))\n",
        "            \n",
        "        return dataset_split\n",
        "\n",
        "def kfoldCV(dataset, f=5, k=5, model=\"logistic\"):\n",
        "    data=cross_validation_split(dataset,f)\n",
        "    result=[]\n",
        "    # determine training and test sets \n",
        "    for i in range(f):\n",
        "        r = list(range(f))\n",
        "        r.pop(i)\n",
        "        for j in r :\n",
        "            if j == r[0]:\n",
        "                cv = data[j]\n",
        "            else:    \n",
        "                cv=np.concatenate((cv,data[j]), axis=0)\n",
        "        \n",
        "        if model == \"logistic\":       \n",
        "            c = logistic(cv[:,0:4],cv[:,4],data[i][:,0:4])\n",
        "            test = c['Y_prediction_test']\n",
        "            \n",
        "        # Menghitung accuracy    \n",
        "        acc=(test == data[i][:,4]).sum()\n",
        "        result.append(acc/len(test))\n",
        "        \n",
        "    return result"
      ],
      "metadata": {
        "id": "AxUyywGGsEYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Menghitung Akurasi**"
      ],
      "metadata": {
        "id": "jrVR_e-5vg9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean \n",
        "\n",
        "print(\"5-Fold Cross Validation\")\n",
        "print(\"Each fold Size:\",int(df.shape[0] / 5))\n",
        "\n",
        "seed(1)\n",
        "acc=kfoldCV(df,5)\n",
        "\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Average of the Accuracy:\", round(mean(acc),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiIVw7dqsNL5",
        "outputId": "894679d3-7a57-458e-9e56-d4658fe511b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-Fold Cross Validation\n",
            "Each fold Size: 30\n",
            "Accuracy: [0.9666666666666667, 0.9, 1.0, 1.0, 0.9666666666666667]\n",
            "Average of the Accuracy: 0.97\n"
          ]
        }
      ]
    }
  ]
}